{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665e5ecc-12d3-4021-87b6-ccd83120d262",
   "metadata": {},
   "source": [
    "# NYC Taxi Data Ingestion Notebook\n",
    "\n",
    "This notebook demonstrates loading NYC taxi data from GitHub and ingesting it into a PostgreSQL database using pandas and SQLAlchemy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe0080-0ee8-4665-b7c1-b9e98a2cf2ae",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Configure Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c580f4-ea80-42d0-a57f-8592ac23e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core data manipulation and database libraries\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "from sqlalchemy import create_engine  # Database connection management\n",
    "from tqdm.auto import tqdm  # Progress bar for loops\n",
    "import warnings  # Warning management\n",
    "import logging  # Error and debug logging\n",
    "\n",
    "# Configure logging for better error tracking\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress non-critical warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad492cf-e0d8-4eab-af5c-5178bf386424",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define parameters for data ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e479d9f8-9f5a-4125-b0b6-bb2b22ff4ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Data Configuration ============\n",
    "# Specify the year and month for the taxi data to download\n",
    "YEAR = 2021  # Data year\n",
    "MONTH = 1    # Data month (January)\n",
    "\n",
    "# Performance tuning parameters\n",
    "CHUNK_SIZE = 100000  # Number of rows to process per iteration\n",
    "\n",
    "# Database table name\n",
    "TABLE_NAME = 'yellow_taxi_data'  # Target table in PostgreSQL\n",
    "\n",
    "# ============ Database Configuration ============\n",
    "# PostgreSQL connection parameters\n",
    "DB_CONFIG = {\n",
    "    'user': 'root',              # Database user\n",
    "    'password': 'root',          # Database password\n",
    "    'host': 'localhost',         # Database host address\n",
    "    'port': '5432',              # PostgreSQL default port\n",
    "    'database': 'ny_taxi'        # Target database name\n",
    "}\n",
    "\n",
    "# ============ Data Source Configuration ============\n",
    "# GitHub repository URL for NYC taxi data\n",
    "URL_PREFIX = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/'\n",
    "\n",
    "# Construct the full data URL with year and month\n",
    "DATA_URL = f'{URL_PREFIX}yellow_tripdata_{YEAR}-{MONTH:02d}.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bedeed68-a3e5-4bd8-888e-32ab7164e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Column Data Types ============\n",
    "# Define explicit data types for each column to ensure proper parsing\n",
    "# Using nullable Int64 instead of int64 to handle missing values\n",
    "DATA_TYPES = {\n",
    "    \"VendorID\": \"Int64\",                    # Taxi company ID (nullable)\n",
    "    \"passenger_count\": \"Int64\",             # Number of passengers (nullable)\n",
    "    \"trip_distance\": \"float64\",             # Trip distance in miles\n",
    "    \"RatecodeID\": \"Int64\",                  # Rate code ID (nullable)\n",
    "    \"store_and_fwd_flag\": \"string\",         # Flag for store-and-forward trips\n",
    "    \"PULocationID\": \"Int64\",                # Pickup location zone ID (nullable)\n",
    "    \"DOLocationID\": \"Int64\",                # Dropoff location zone ID (nullable)\n",
    "    \"payment_type\": \"Int64\",                # Payment method ID (nullable)\n",
    "    \"fare_amount\": \"float64\",               # Base fare amount\n",
    "    \"extra\": \"float64\",                     # Extra charges\n",
    "    \"mta_tax\": \"float64\",                   # MTA tax amount\n",
    "    \"tip_amount\": \"float64\",                # Tip amount\n",
    "    \"tolls_amount\": \"float64\",              # Toll charges\n",
    "    \"improvement_surcharge\": \"float64\",     # Improvement surcharge\n",
    "    \"total_amount\": \"float64\",              # Total trip amount\n",
    "    \"congestion_surcharge\": \"float64\"       # Congestion surcharge\n",
    "}\n",
    "\n",
    "# ============ Date Columns ============\n",
    "# Specify columns that should be parsed as datetime\n",
    "PARSE_DATES = [\n",
    "    \"tpep_pickup_datetime\",     # Pickup timestamp\n",
    "    \"tpep_dropoff_datetime\"     # Dropoff timestamp\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38623174-03a4-4b5c-a16e-784ce9b87230",
   "metadata": {},
   "source": [
    "## Load and Explore Data\n",
    "\n",
    "Load a sample of the data and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6b8761-d12c-45c7-9c6c-dbd57fbe55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data shape: (1000, 18)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01 00:30:10</td>\n",
       "      <td>2021-01-01 00:36:12</td>\n",
       "      <td>1</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>142</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01 00:51:20</td>\n",
       "      <td>2021-01-01 00:52:19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>238</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01 00:43:30</td>\n",
       "      <td>2021-01-01 01:11:06</td>\n",
       "      <td>1</td>\n",
       "      <td>14.70</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>51.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01 00:15:48</td>\n",
       "      <td>2021-01-01 00:31:01</td>\n",
       "      <td>0</td>\n",
       "      <td>10.60</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>138</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>36.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-01 00:31:49</td>\n",
       "      <td>2021-01-01 00:48:21</td>\n",
       "      <td>1</td>\n",
       "      <td>4.94</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>68</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>24.36</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2021-01-01 00:30:10   2021-01-01 00:36:12                1   \n",
       "1         1  2021-01-01 00:51:20   2021-01-01 00:52:19                1   \n",
       "2         1  2021-01-01 00:43:30   2021-01-01 01:11:06                1   \n",
       "3         1  2021-01-01 00:15:48   2021-01-01 00:31:01                0   \n",
       "4         2  2021-01-01 00:31:49   2021-01-01 00:48:21                1   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           2.10           1                  N           142            43   \n",
       "1           0.20           1                  N           238           151   \n",
       "2          14.70           1                  N           132           165   \n",
       "3          10.60           1                  N           138           132   \n",
       "4           4.94           1                  N            68            33   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2          8.0    3.0      0.5        0.00           0.0   \n",
       "1             2          3.0    0.5      0.5        0.00           0.0   \n",
       "2             1         42.0    0.5      0.5        8.65           0.0   \n",
       "3             1         29.0    0.5      0.5        6.05           0.0   \n",
       "4             1         16.5    0.5      0.5        4.06           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  \n",
       "0                    0.3         11.80                   2.5  \n",
       "1                    0.3          4.30                   0.0  \n",
       "2                    0.3         51.95                   0.0  \n",
       "3                    0.3         36.35                   0.0  \n",
       "4                    0.3         24.36                   2.5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a sample of the data for quick inspection\n",
    "# Using nrows=1000 to load only 1000 rows for faster processing\n",
    "# This allows us to inspect the data structure without loading the entire dataset\n",
    "df_sample = pd.read_csv(\n",
    "    DATA_URL,\n",
    "    dtype=DATA_TYPES,           # Apply defined data types\n",
    "    parse_dates=PARSE_DATES,    # Parse date columns\n",
    "    nrows=1000                  # Load sample for quick inspection\n",
    ")\n",
    "\n",
    "# Display basic information about the loaded sample\n",
    "print(f\"Sample data shape: {df_sample.shape}\")  # Show number of rows and columns\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_sample.head()  # Display first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1657d13-0382-4d77-919d-bedc6c0773e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VendorID                          Int64\n",
       "tpep_pickup_datetime     datetime64[ns]\n",
       "tpep_dropoff_datetime    datetime64[ns]\n",
       "passenger_count                   Int64\n",
       "trip_distance                   float64\n",
       "RatecodeID                        Int64\n",
       "store_and_fwd_flag       string[python]\n",
       "PULocationID                      Int64\n",
       "DOLocationID                      Int64\n",
       "payment_type                      Int64\n",
       "fare_amount                     float64\n",
       "extra                           float64\n",
       "mta_tax                         float64\n",
       "tip_amount                      float64\n",
       "tolls_amount                    float64\n",
       "improvement_surcharge           float64\n",
       "total_amount                    float64\n",
       "congestion_surcharge            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display data types for each column\n",
    "# This verifies that all columns were loaded with the correct types\n",
    "print(\"\\nData types:\")\n",
    "df_sample.dtypes  # Show data type for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab37a4eb-9ecb-4c7c-9259-853b72ea8cc1",
   "metadata": {},
   "source": [
    "## Database Setup\n",
    "\n",
    "Create database connection and table schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6b1278-7949-47dd-9e1d-eea83483477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database engine created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Construct the PostgreSQL connection URL from configuration parameters\n",
    "# Format: postgresql://user:password@host:port/database\n",
    "db_url = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "\n",
    "# Create SQLAlchemy engine for database connection management\n",
    "# This engine handles all interactions with the PostgreSQL database\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Verify connection was successful\n",
    "print(\"Database engine created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f91a6bf-0581-4333-9aa8-a221ec650764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 17:20:42,835 - INFO - Generating database schema...\n",
      "2026-01-16 17:20:42,903 - INFO - Schema generated successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated database schema:\n",
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count BIGINT, \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" BIGINT, \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate SQL schema from the sample dataframe\n",
    "# This creates the CREATE TABLE statement based on our data types\n",
    "try:\n",
    "    logger.info(\"Generating database schema...\")\n",
    "    schema = pd.io.sql.get_schema(df_sample, name=TABLE_NAME, con=engine)\n",
    "    \n",
    "    # Display the generated schema for review\n",
    "    print(\"Generated database schema:\")\n",
    "    print(schema)\n",
    "    logger.info(\"Schema generated successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating schema: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95658a95-65f2-4ee4-90c5-9e8a20475c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 17:20:42,909 - INFO - Creating table 'yellow_taxi_data'...\n",
      "2026-01-16 17:20:43,107 - INFO - Table 'yellow_taxi_data' created successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Table 'yellow_taxi_data' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create an empty table in PostgreSQL with the correct schema\n",
    "# df_sample.head(n=0) returns an empty dataframe with the same structure\n",
    "# if_exists='replace' will drop the table if it already exists and recreate it\n",
    "try:\n",
    "    logger.info(f\"Creating table '{TABLE_NAME}'...\")\n",
    "    df_sample.head(n=0).to_sql(name=TABLE_NAME, con=engine, if_exists='replace')\n",
    "    \n",
    "    # Confirm table creation\n",
    "    logger.info(f\"Table '{TABLE_NAME}' created successfully!\")\n",
    "    print(f\"‚úì Table '{TABLE_NAME}' created successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating table: {str(e)}\")\n",
    "    print(f\"‚ùå Table creation error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a95371-b811-43c0-bf0e-8bd487ecd9c0",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Load complete dataset in chunks and insert into the database.\n",
    "\n",
    "### Error Handling\n",
    "- **Connection Errors**: Database connection issues will raise an exception\n",
    "- **Data Errors**: Invalid data formats will be logged and skipped\n",
    "- **Network Errors**: Download failures will attempt to retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad83a72-f703-47d3-8116-e6aeb9a911bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 17:20:43,130 - INFO - Starting data ingestion into 'yellow_taxi_data'...\n",
      "2026-01-16 17:20:43,132 - INFO - Scanning data file to calculate total chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting data into 'yellow_taxi_data' table...\n",
      "Chunk size: 100,000 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 17:20:44,171 - INFO - Total rows: 1,369,765, Expected chunks: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb40c9470544d368476d5b4a5ea7fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inserting chunk 0/14 |                                          | 0.0% [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 17:22:25,078 - INFO - Ingestion complete: 1,369,765 rows, 0 errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "‚úì Data ingestion completed successfully!\n",
      "==================================================\n",
      "Total rows inserted: 1,369,765\n",
      "Total chunks processed: 14/14\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data ingestion process with status messages\n",
    "try:\n",
    "    logger.info(f\"Starting data ingestion into '{TABLE_NAME}'...\")\n",
    "    print(f\"Ingesting data into '{TABLE_NAME}' table...\")\n",
    "    print(f\"Chunk size: {CHUNK_SIZE:,} rows\\n\")\n",
    "    \n",
    "    # First, get the total number of rows to calculate expected chunks\n",
    "    logger.info(\"Scanning data file to calculate total chunks...\")\n",
    "    df_preview = pd.read_csv(\n",
    "        DATA_URL,\n",
    "        usecols=['VendorID']  # Only read one column for speed (no parsing needed)\n",
    "    )\n",
    "    total_rows = len(df_preview)\n",
    "    total_chunks = (total_rows + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "    logger.info(f\"Total rows: {total_rows:,}, Expected chunks: {total_chunks}\")\n",
    "    del df_preview  # Free memory\n",
    "    \n",
    "    # Create an iterator that reads the CSV file in chunks\n",
    "    # This approach is memory-efficient for large files (>1GB)\n",
    "    # The iterator reads CHUNK_SIZE rows at a time\n",
    "    df_iterator = pd.read_csv(\n",
    "        DATA_URL,\n",
    "        dtype=DATA_TYPES,           # Apply defined data types\n",
    "        parse_dates=PARSE_DATES,    # Parse date columns\n",
    "        iterator=True,              # Enable iterator mode for chunked reading\n",
    "        chunksize=CHUNK_SIZE        # Read this many rows per iteration\n",
    "    )\n",
    "    \n",
    "    # Track total rows inserted for final reporting\n",
    "    rows_inserted = 0\n",
    "    errors_count = 0\n",
    "    \n",
    "    # Custom format for tqdm progress bar showing chunk number and total\n",
    "    # Format: \"Inserting chunk 3/14 % [00:15<00:08, 1.23s/chunk]\"\n",
    "    bar_format = \"Inserting chunk {n_fmt}/{total_fmt} |{bar}|{percentage:.1f}% [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "    \n",
    "    # Iterate through each chunk and insert into the database\n",
    "    # Using tqdm with custom format to show \"Inserting chunk X/Y\"\n",
    "    for chunk_idx, chunk in enumerate(tqdm(\n",
    "        df_iterator, \n",
    "        desc=\"üìä\",  # Emoji description\n",
    "        unit=\"chunk\",  # Unit label\n",
    "        total=total_chunks,  # Show total chunks\n",
    "        bar_format=bar_format,  # Custom format\n",
    "        ncols=90  # Terminal width\n",
    "    ), 1):\n",
    "        try:\n",
    "            # Append each chunk to the database table\n",
    "            # if_exists='append' adds rows to the existing table\n",
    "            chunk.to_sql(name=TABLE_NAME, con=engine, if_exists='append', index=False)\n",
    "            \n",
    "            # Update row counter for progress tracking\n",
    "            rows_inserted += len(chunk)\n",
    "            logger.debug(f\"Chunk {chunk_idx}/{total_chunks}: Inserted {len(chunk)} rows\")\n",
    "            \n",
    "        except Exception as chunk_error:\n",
    "            errors_count += 1\n",
    "            logger.error(f\"Error inserting chunk {chunk_idx}: {str(chunk_error)}\")\n",
    "            # Continue processing remaining chunks despite errors\n",
    "            continue\n",
    "    \n",
    "    # Display completion status and summary statistics\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"‚úì Data ingestion completed successfully!\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total rows inserted: {rows_inserted:,}\")\n",
    "    print(f\"Total chunks processed: {chunk_idx}/{total_chunks}\")\n",
    "    if errors_count > 0:\n",
    "        print(f\"‚ö† Errors encountered: {errors_count} chunk(s)\")\n",
    "    \n",
    "    logger.info(f\"Ingestion complete: {rows_inserted:,} rows, {errors_count} errors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Critical error during ingestion: {str(e)}\")\n",
    "    print(f\"\\n‚ùå INGESTION FAILED\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
